/*
 * Copyright (C) 2010 Minwoo Jeong (minwoo.j@gmail.com).
 * This file is part of the "TriCRF" distribution.
 * http://github.com/minwoo/TriCRF/
 * This software is provided under the terms of Modified BSD license: see LICENSE for the detail.
 */

/// max headers
#include "TriCRF2.h"
#include "Evaluator.h"
#include "Utility.h"
#include "LBFGS.h"
/// standard headers
#include <cassert>
#include <cfloat>
#include <cmath>
#include <limits>
#include <algorithm>
#include <stdexcept>
#include <iostream>
#include <fstream>

/// for fast accessing the element of matrixes
#define MAT3(I, X, Y)			((m_state_size * m_state_size * (I)) + (m_state_size * (X)) + Y)
#define MAT2(I, X)				((m_state_size * (I)) + X)
#define MATZ2(Z, I, X)		((m_seq_size * m_state_size * Z) + (m_state_size * (I)) + X)
/// for fast accessing the element of matrixes (in TriCRF22)
#define TCRF2_MAT3(SIZE, I, X, Y)			((SIZE * SIZE * (I)) + (SIZE * (X)) + Y)
#define TCRF2_MAT2(SIZE, I, X)				((SIZE * (I)) + X)
/// for fast accessing the element of matrixes (in TriCRF21)
#define ZMAT3(Z, I, X, Y)	((m_state_size[Z] * m_state_size[Z] * (I)) + (m_state_size[Z] * (X)) + Y)
#define ZMAT2(Z, I, X)		((m_state_size[Z] * (I)) + X)
#define MAT(I, X)				((m_state_size2 * (I)) + X)

using namespace std;

namespace tricrf {

///////////////////////////////////////////////////////////////////////////////////////////////////////////////
//
// MODEL-2: factorizing (y,z) feature 
//
///////////////////////////////////////////////////////////////////////////////////////////////////////////////

/** Constructor.
*/
TriCRF2::TriCRF2() {
	m_default_oid = 0;
}

/** Constructor with logger.
*/
TriCRF2::TriCRF2(Logger *logger) {
	setLogger(logger);
	logger->report(2, MAX_HEADER);
	logger->report(2, ">> Triangular-chain Conditional Random Fields (Model2) << \n\n");
	m_default_oid = 0;
}

void TriCRF2::clear() {
	m_ParamSeq.clear();
	m_ParamTopic.clear();
}

void TriCRF2::initializeModel() {
	m_ParamSeq.initialize();
	m_ParamTopic.initialize();
}

/** Save the model.
	@param filename file to be saved 
	@return success or fail
*/
bool TriCRF2::saveModel(const std::string& filename) {
	/// Checking the error
	if (filename == "")
		return false;

	timer stop_watch;
	logger->report("[Model saving]\n");

	/// file stream
    ofstream f(filename.c_str());
    f.precision(20);
    if (!f)
        throw runtime_error("unable to open file to write");

    /// header
    f << "# MAX: A C++ Library for Structured Prediction" << endl;
	f << "# TriCRF2 Model file (text format)" << endl;
	f << "# Do not edit this file" << endl;
	f << "# " << endl << ":" << endl;
	
	if (!m_ParamTopic.save(f))
		return false;
	if (!m_ParamSeq.save(f))
		return false;
	f.close();

	logger->report("  saving time = \t%.3f\n\n", stop_watch.elapsed());

	return true;
}

/** Load the model.
	@param filename file to be loaded
	@return success or fail
*/
bool TriCRF2::loadModel(const std::string& filename) {
	/// Checking the error
	if (filename == "")
		return false;

	timer stop_watch;
	logger->report("[Model loading]\n");

	/// file stream
    ifstream f(filename.c_str());
    f.precision(20);
    if (!f)
        throw runtime_error("fail to open model file");

    /// header
	size_t count = 0;
    string line;
    getline(f, line);
    while (line.empty() || line[0] == '#') {
		if (count == 1) {
			vector<string> tok = tokenize(line);
			if (tok.size() < 2 || tok[1] != "TriCRF2") {
				logger->report("|Error| Invalid model files ... \n");
				return false;
			}
		}
        getline(f, line);
		count++;
	}

	if (!m_ParamTopic.load(f))
		return false;
	logger->report("  >>Parameters for topic features\n");
	m_ParamTopic.print(logger);

	if (!m_ParamSeq.load(f))
		return false;
	logger->report("  >>Parameters for sequence features\n");
	m_ParamSeq.print(logger);

	f.close();
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());

	m_ParamTopic.makeStateIndex(false);
	m_ParamSeq.makeStateIndex();
	m_state_size = m_ParamSeq.sizeStateVec();
	m_topic_size = m_ParamTopic.sizeStateVec();

	createIndex();

	return true;
}

/**	Read the data from file
*/
void TriCRF2::readTrainData(const string& filename) {

	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");
	
	/// initializing
	TriSequence triseq;
	size_t count = 0;
	string prev_label = "";
	string topic;
	timer stop_watch;
	logger->report("[Training data file loading]\n");
	m_TrainSet.clear();
	m_TrainSetCount.clear();

	/// To reduce the storage and computation
	map<vector<vector<string> >, size_t> train_data_map;
	vector<vector<string> > token_list;

	size_t seq_count = 0;
	while (getline(f,line)) {
		vector<string> tokens = tokenize(line, " \t");
		if (line.empty() || tokens.size() <= 0) {	 ///< sequence break
			if (train_data_map.find(token_list) == train_data_map.end()) {
				m_TrainSet.append(triseq);
				train_data_map.insert(make_pair(token_list, m_TrainSetCount.size()));
				m_TrainSetCount.push_back(1.0);
			} else {
				m_TrainSetCount[train_data_map[token_list]] += 1.0;
			}
			triseq.seq.clear();
			token_list.clear();
			prev_label = "";
			seq_count = 0;
			++count;
		} else {
			++seq_count;
			token_list.push_back(tokens);
			if (seq_count == 1) { ///< this is a topic 
				triseq.topic = packEvent(tokens, &m_ParamTopic);	///< wanrning: There are no common element in topic classes and sequence classes.
				topic = tokens[0];
			} else {
				Event ev = packEvent(tokens,  &m_ParamSeq);	///< observation features
				triseq.seq.push_back(ev);	///< append

				/// State transition features
				/// This can be extended to state-dependent observation features. (See Sutton and McCallum, 2006)
				if (prev_label != "") {
					size_t pid = m_ParamSeq.addNewObs("@" + prev_label);
					m_ParamSeq.updateParam(ev.label, pid, ev.fval);
				}
				/// Topic-Sequence state features
				/// (See Jeong and Lee, 2006 and Jeong and Lee, 2007)
				size_t pid = m_ParamTopic.addNewObs("@" + topic);
				m_ParamTopic.updateParam(ev.label, pid, ev.fval);

				prev_label = tokens[0];
			}
		}	// else

	}	// while
	m_ParamTopic.endUpdate();
	m_ParamSeq.endUpdate(); 

	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());
	
	m_ParamSeq.makeStateIndex();
	m_ParamTopic.makeStateIndex(false);
	m_state_size = m_ParamSeq.sizeStateVec();
	m_topic_size = m_ParamTopic.sizeStateVec();
}

/**	Read the data from file
*/
void TriCRF2::readDevData(const string& filename) {

	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");
	
	/// initializing
	TriSequence triseq;
	size_t count = 0;
	string prev_label = "";
	string topic;
	timer stop_watch;
	logger->report("[Dev data file loading]\n");
	m_DevSet.clear();
	m_DevSetCount.clear();

	/// To reduce the storage and computation
	map<vector<vector<string> >, size_t> dev_data_map;
	vector<vector<string> > token_list;

	size_t seq_count = 0;
	while (getline(f,line)) {
		vector<string> tokens = tokenize(line, " \t");
		if (line.empty() || tokens.size() <= 0) {	 ///< sequence break
			if (dev_data_map.find(token_list) == dev_data_map.end()) {
				m_DevSet.append(triseq);
				dev_data_map.insert(make_pair(token_list, m_DevSetCount.size()));
				m_DevSetCount.push_back(1.0);
			} else {
				m_DevSetCount[dev_data_map[token_list]] += 1.0;
			}
			triseq.seq.clear();
			token_list.clear();
			prev_label = "";
			seq_count = 0;
			++count;
		} else {
			++seq_count;
			token_list.push_back(tokens);
			if (seq_count == 1) { ///< this is a topic 
				triseq.topic = packEvent(tokens, &m_ParamTopic, true);	///< wanrning: There are no common element in topic classes and sequence classes.
				topic = tokens[0];
			} else {
				Event ev = packEvent(tokens,  &m_ParamSeq, true);	///< observation features
				triseq.seq.push_back(ev);	///< append

				prev_label = tokens[0];
			}
		}	// else

	}	// while

	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());

}

void TriCRF2::calculateEdge() {
	double* theta_seq = m_ParamSeq.getWeight();
	m_M.resize(m_state_size * m_state_size);
	fill(m_M.begin(), m_M.end(), 1.0);

	vector<StateParam>::iterator iter = m_ParamSeq.m_StateIndex.begin();
	for (; iter != m_ParamSeq.m_StateIndex.end(); ++iter) {
		m_M[MAT2(iter->y1,iter->y2)] *= exp(theta_seq[iter->fid] * iter->fval);	 
	}
}


/**	Calculate the factors.
	References 
		Jeong and Lee, Triangular-chain Conditional Random Fields, (Submitted), IEEE TASLP.
*/
void TriCRF2::calculateFactors(TriSequence &triseq) {
	/// Initialization
	m_seq_size = triseq.seq.size() + 1;	///< sequence length
	double* theta_seq = m_ParamSeq.getWeight();
	double* theta_topic = m_ParamTopic.getWeight();

	/// Factor matrix initialization
	m_R.resize(m_seq_size * m_state_size);
	m_Z.resize(m_topic_size * m_state_size);
	fill(m_R.begin(), m_R.end(), 1.0);
	fill(m_Z.begin(), m_Z.end(), 1.0);

	/// Calculation
	for (size_t i = 0; i < m_seq_size-1; i++) {
		/// Observation factor
		vector<ObsParam> obs_param = m_ParamSeq.makeObsIndex(triseq.seq[i].obs);
		vector<ObsParam>::iterator iter = obs_param.begin();
		for(; iter != obs_param.end(); ++iter) {
			m_R[MAT2(i, iter->y)] *= exp(theta_seq[iter->fid] * iter->fval);
		}
		
		/// State factor
		//if (i > 0) {
		//	vector<StateParam>::iterator iter = m_ParamSeq.m_StateIndex.begin();
		//	for (; iter != m_ParamSeq.m_StateIndex.end(); ++iter) {
		//		m_M[MAT3(i,iter->y1,iter->y2)] *= exp(theta_seq[iter->fid] * iter->fval);	 
		//	}
		//} ///< if

	}	///< for 

	/// Topic factor
	vector<StateParam>::iterator iter = m_ParamTopic.m_StateIndex.begin();
	for (; iter != m_ParamTopic.m_StateIndex.end(); ++iter) {
		m_Z[MAT2(iter->y1, iter->y2)] *= exp(theta_topic[iter->fid] * iter->fval);
	}

	/// Gamma 
	m_Gamma.resize(m_topic_size, 1.0);
	fill(m_Gamma.begin(), m_Gamma.end(), 1.0);
	
	vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(triseq.topic.obs);
	vector<ObsParam>::iterator iter2 = obs_param.begin();
	for(; iter2 != obs_param.end(); ++iter2) {
		m_Gamma[iter2->y] *= exp(theta_topic[iter2->fid] * iter2->fval);
	}
}

/**	Calculate the factors.
	References 
		Jeong and Lee, Triangular-chain Conditional Random Fields, (Submitted), IEEE TASLP.
*/
void TriCRF2::calculateFactors(TriStringSequence &triseq) {
	/// Initialization
	m_seq_size = triseq.seq.size() + 1;	///< sequence length
	double* theta_seq = m_ParamSeq.getWeight();
	double* theta_topic = m_ParamTopic.getWeight();

	/// Factor matrix initialization
	m_R.resize(m_seq_size * m_state_size);
	m_Z.resize(m_topic_size * m_state_size);
	fill(m_R.begin(), m_R.end(), 1.0);
	fill(m_Z.begin(), m_Z.end(), 1.0);

	/// Calculation
	for (size_t i = 0; i < m_seq_size-1; i++) {
		/// Observation factor
		vector<ObsParam> obs_param = m_ParamSeq.makeObsIndex(triseq.seq[i].obs);
		vector<ObsParam>::iterator iter = obs_param.begin();
		for(; iter != obs_param.end(); ++iter) {
			m_R[MAT2(i, iter->y)] *= exp(theta_seq[iter->fid] * iter->fval);
		}
		
		/// State factor
		//if (i > 0) {
		//	vector<StateParam>::iterator iter = m_ParamSeq.m_StateIndex.begin();
		//	for (; iter != m_ParamSeq.m_StateIndex.end(); ++iter) {
		//		m_M[MAT3(i,iter->y1,iter->y2)] *= exp(theta_seq[iter->fid] * iter->fval);	 
		//	}
		//} ///< if

	}	///< for 

	/// Topic factor
	vector<StateParam>::iterator iter = m_ParamTopic.m_StateIndex.begin();
	for (; iter != m_ParamTopic.m_StateIndex.end(); ++iter) {
		m_Z[MAT2(iter->y1, iter->y2)] *= exp(theta_topic[iter->fid] * iter->fval);
	}

	/// Gamma 
	m_Gamma.resize(m_topic_size, 1.0);
	fill(m_Gamma.begin(), m_Gamma.end(), 1.0);
	
	vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(triseq.topic.obs);
	vector<ObsParam>::iterator iter2 = obs_param.begin();
	for(; iter2 != obs_param.end(); ++iter2) {
		m_Gamma[iter2->y] *= exp(theta_topic[iter2->fid] * iter2->fval);
	}
}

/**	Forward Recursion.
	Computing and storing the alpha value.
*/
void TriCRF2::forward() {
	//m_Alpha.resize(m_topic_size* m_seq_size * m_state_size);
	//fill(m_Alpha.begin(), m_Alpha.end(), 0.0);
	m_Alpha.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		m_Alpha[z].resize(m_seq_size * m_zy_size[z]);
		fill(m_Alpha[z].begin(), m_Alpha[z].end(), 0.0);
	}

	for (size_t z = 0; z < m_topic_size; z++) {
		//for (size_t j = 0; j < m_state_size; j++) {
		for (vector<StateParam>::iterator iter = m_y_state[z].begin(); iter != m_y_state[z].end(); ++iter) {
			size_t j = iter->y2;
			long double prob = m_R[MAT2(0, j)] * m_M[MAT2(m_default_oid, j)];
			m_Alpha[z][TCRF2_MAT2(m_zy_size[z], 0, iter->y1)] += prob * m_Z[MAT2(z, j)];
		}
	}

	for (size_t z = 0; z < m_topic_size; z++) {
		for (size_t i = 1; i < m_seq_size; i++) {
			//for (size_t j = 0; j < m_state_size; j++) {
			for (vector<StateParam>::iterator iter = m_y_state[z].begin(); iter != m_y_state[z].end(); ++iter) {
				size_t j = iter->y2;
				long double prob = m_R[MAT2(i, j)] * m_Z[MAT2(z, j)];
				if (prob > 0) {
					//for (size_t k = 0; k < m_state_size; k++) {
					for (vector<StateParam>::iterator iter2 = m_y_state[z].begin(); iter2 != m_y_state[z].end(); ++iter2) {
						size_t k = iter2->y2;
						m_Alpha[z][TCRF2_MAT2(m_zy_size[z],i, iter->y1)] += 
										m_Alpha[z][TCRF2_MAT2(m_zy_size[z], i-1, iter2->y1)] * m_M[MAT2(k, j)] * prob;
					} ///< for k
				} // if prob > 0
			} ///< for j
		}  ///< for i
	}  ///< for z

}

/**	Backward Recursion.
	Computing and storing the beta value.
*/
void TriCRF2::backward() {
	//m_Beta.resize(m_topic_size * m_seq_size * m_state_size);
	//fill(m_Beta.begin(), m_Beta.end(), 0.0);
	m_Beta.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		m_Beta[z].resize(m_seq_size * m_zy_size[z]);
		fill(m_Beta[z].begin(), m_Beta[z].end(), 0.0);
	}

	for (size_t z = 0; z < m_topic_size; z++) {
		m_Beta[z][TCRF2_MAT2(m_zy_size[z], m_seq_size-1, m_y_state[z][0].y1)] = 1.0;
	}

	//for (size_t z = 0; z < m_topic_size; z++) { // original
	for (size_t prune = 0; prune < m_prune.size(); prune++) {
		size_t z = m_prune[prune].second;

		for (size_t i = m_seq_size-1; i >= 1; i--) {
			//for (size_t k = 0; k < m_state_size; k++) {
			for (vector<StateParam>::iterator iter = m_y_state[z].begin(); iter != m_y_state[z].end(); ++iter) {
				size_t k = iter->y2;
				long double prob = m_R[MAT2(i, k)] * m_Z[MAT2(z, k)]; 
				if (prob > 0) {
					//for (size_t j = 0; j < m_state_size; j++) {
					for (vector<StateParam>::iterator iter2 = m_y_state[z].begin(); iter2 != m_y_state[z].end(); ++iter2) {
						size_t j = iter2->y2;
						m_Beta[z][TCRF2_MAT2(m_zy_size[z], i-1, iter2->y1)] += 
										m_Beta[z][TCRF2_MAT2(m_zy_size[z], i, iter->y1)] * m_M[MAT2(j, k)] * prob;
					} ///< for j
				} ///< if prob > 0
            } ///< for k
        } ///< for i
    } ///< for z

}

/**	Partition function (Z).
	@return normalizing constant 
*/
long double TriCRF2::getPartitionZ() {
	m_prune.clear();
	long double zval = 0.0;

	for (size_t z = 0; z < m_topic_size; z++) {
		long double prob = m_Alpha[z][TCRF2_MAT2(m_zy_size[z], m_seq_size-1, m_y_state[z][0].y1)] * m_Gamma[z];
		zval += prob;
		m_prune.push_back(make_pair(prob, z));
	}
	/// for pruning
	for (size_t z = 0; z < m_topic_size; z++) {
		m_prune[z].first /= zval;
	}
	sort(m_prune.rbegin(), m_prune.rend());

	return zval;
}

/** Calculate prob. of y* sequence.
	@param seq			given data (y, x)
	@return probability
*/
long double TriCRF2::calculateProb(TriSequence& triseq) {
	long double z = getPartitionZ();

    long double seq_prob = 1.0;
						[[maybe_unused]] size_t prev_y = m_default_oid;
    size_t y;
    for (size_t i=0; i < m_seq_size; i++) {
        if (i < m_seq_size-1) {
            y = triseq.seq[i].label;
        } else {
            y = m_y_state[triseq.topic.label][0].y2;
        }
        seq_prob *= m_R[MAT2(i,y)] * m_M[MAT2(prev_y,y)] * m_Z[MAT2(triseq.topic.label, y)];
        prev_y = y;
       
    }
    if (seq_prob == 0.0) {
        cerr << "seq_prob==0 ";
    }

    return seq_prob * m_Gamma[triseq.topic.label] / z;
}

/** Viterbi search to find the best probable output sequence.
  Viterbi algorithm.
 @param prob		dummy probability vector
 @return outcome sequence
*/
vector<size_t> TriCRF2::viterbiSearch(size_t& max_z, long double& prob) {
	/// Initialization
	long double max_prob = -10000.0;
	max_z = m_default_oid;
	vector<size_t> max_y;

	vector<size_t> psi_x(m_state_size) ;
	vector<long double> delta_x(m_state_size);
	fill(psi_x.begin(), psi_x.end(), m_state_size);
	fill(delta_x.begin(), delta_x.begin(), 0.0);

	/// Search
	///for (size_t z = 0; z < m_topic_size; z++) {
	for (size_t prune = 0; prune < m_prune.size(); prune++) {
		size_t z = m_prune[prune].second;

		vector<vector<size_t> > psi;
		vector<vector<long double> > delta;

		for (size_t i=0; i < m_seq_size; i++) {
			vector<size_t> psi_i= psi_x;
			vector<long double> delta_i = delta_x;

			//for (size_t j=0; j < m_state_size; j++) {
			for (vector<StateParam>::iterator iter = m_y_state[z].begin(); iter != m_y_state[z].end(); ++iter) {
				size_t j = iter->y2;
				long double max = -10000.0;
				size_t max_k = m_y_state[z][0].y2;
				if (i == 0) {
					max = m_R[MAT2(i, j)] * m_M[MAT2(m_default_oid, j)] * m_Z[MAT2(z, j)];
					max_k = m_default_oid;
				} else {
					long double p = m_R[MAT2(i,j)] * m_Z[MAT2(z, j)];
					//for (size_t k=0; k < m_state_size; k++) {
					for (vector<StateParam>::iterator iter2 = m_y_state[z].begin(); iter2 != m_y_state[z].end(); ++iter2) {
						size_t k = iter2->y2;
						double val = delta[i-1][k] *  m_M[MAT2(k,j)] * p;
						if (val > max) {
							max = val;
							max_k = k;
						}
					}
				}
				
				delta_i[j] = max;
				psi_i[j] = max_k;
				//delta_i.push_back(max);
				//psi_i.push_back(max_k);
			}
			delta.push_back(delta_i);
			psi.push_back(psi_i);
		} ///< for each i

		/// Back-tracking
		vector<size_t> y_seq;
		size_t prev_y = m_y_state[z][0].y2;
		for (size_t i = m_seq_size-1; i >= 1; i--) {
			//cout << prev_y << " " << psi[i][prev_y] << endl;
			size_t y = psi[i][prev_y];
			y_seq.push_back(y);
			prev_y = y;
		}
		reverse(y_seq.begin(), y_seq.end());
		double tmp_prob = delta[m_seq_size-1][m_y_state[z][0].y2] * m_Gamma[z];
		
		if (tmp_prob > max_prob) {
			max_prob = tmp_prob;
			max_z = z;
			max_y = y_seq;
		}

	} ///< for each z
	prob = max_prob;
	return max_y;

}

void TriCRF2::createIndex() {
	/// Creating Z-Y indexes for reduced search space
	/// todo: remove the makeStateIndex(z)
	m_y_state.clear();
	m_zy_index.clear();
	m_zy_size.clear();
	m_zy_index.resize(m_topic_size);
	m_yz_index.resize(m_topic_size);
	for (size_t z = 0; z < m_topic_size; z++) {
		vector<StateParam> y_state = m_ParamTopic.makeStateIndex(z);
		m_zy_size.push_back(y_state.size());
		m_zy_index[z].resize(m_state_size);
		m_yz_index[z].resize(m_state_size);
		fill(m_zy_index[z].begin(), m_zy_index[z].end(), m_state_size);
		for (vector<StateParam>::iterator iter = y_state.begin(); iter != y_state.end(); ++iter) {
			m_zy_index[z][iter->y2] = iter->y1;
			m_yz_index[z][iter->y1] = iter->y2;
		}
		m_y_state.push_back(y_state);
	}
}

/** Training with LBFGS optimizer.
	@param max_iter	maximum number of iteration
	@param sigma	Gaussian prior variance
*/
bool TriCRF2::estimateWithLBFGS(size_t max_iter, double sigma, bool L1, double eta) {
	LBFGS lbfgs;	///< LBFGS optimizer

	/// Parameter weight setting
	size_t n_theta = m_ParamTopic.size() + m_ParamSeq.size();
	double* theta = new double[n_theta];

	double* theta_topic = m_ParamTopic.getWeight();
	double* theta_seq = m_ParamSeq.getWeight();
	size_t tmp_i = 0;
	for (; tmp_i < m_ParamTopic.size(); ++tmp_i) 
		theta[tmp_i] = theta_topic[tmp_i];
	for (size_t i = 0; i < m_ParamSeq.size(); ++i, ++tmp_i) 
		theta[tmp_i] = theta_seq[i];

	double* gradient = new double[n_theta]; //m_Param.getGradient();
	double* gradient_topic = m_ParamTopic.getGradient();
	double* gradient_seq = m_ParamSeq.getGradient();

	Evaluator eval1(m_ParamTopic, false);		///< Evaluator (topic)
	Evaluator eval2(m_ParamSeq);		///< Evaluator (sequence)
	timer t;		///< timer

	/// Reporting
	logger->report("[Parameter estimation]\n");
	logger->report("  Method = \t\tLBFGS\n");
	logger->report("  Regularization = \t%s\n", (sigma ? (L1 ? "L1":"L2") : "none"));
	logger->report("  Penalty value = \t%.2f\n\n", sigma);
	logger->report("  >>Parameters for topic features\n");
	m_ParamTopic.print(logger);
	logger->report("  >>Parameters for sequence features\n");
	m_ParamSeq.print(logger);
	logger->report("[Iterations]\n");
	logger->report("%4s %15s %8s %8s %8s %8s\n", "iter", "loglikelihood", "acc", "micro-f1", "macro-f1", "sec");
	
	double old_obj = 1e+37;
	int converge = 0;

	[[maybe_unused]] double time_for_factor = 0.0;
	[[maybe_unused]] double time_for_forward = 0.0;
	[[maybe_unused]] double time_for_backward = 0.0;
	[[maybe_unused]] double time_for_evaluation = 0.0;
	[[maybe_unused]] double time_for_estimating = 0.0;
	
	createIndex();

	/// Training iteration
    for (size_t niter = 0 ;niter < max_iter; ++niter) {

		/// Initializing local variables
        timer t2;	///< elapsed time for one iteration
		m_ParamTopic.initializeGradient();	///< gradient vector initialization
		m_ParamSeq.initializeGradient();

		eval1.initialize();	///< evaluator intialization
		eval2.initialize(); 

		calculateEdge();
		
		/// for each training set
        vector<TriSequence>::iterator it = m_TrainSet.begin();
		vector<double>::iterator count_it = m_TrainSetCount.begin();
        for (; it != m_TrainSet.end(); ++it, ++count_it) {
			double count = *count_it;
			/// Forward-Backward  
			timer stop_watch;
			calculateFactors(*it);
			time_for_factor += stop_watch.elapsed();
			stop_watch.restart();
  			forward();
			time_for_forward += stop_watch.elapsed();
						[[maybe_unused]] long double zval = getPartitionZ();

			////////////////////////////////////////////////////////////////////
			/// pruning
			////////////////////////////////////////////////////////////////////
			long double threshold = m_prune[0].first / m_prune_threshold;
			if (niter > 0) {
				vector<pair<long double, size_t> >::iterator pit = m_prune.begin();
				for (; pit != m_prune.end(); pit++) {
					if (pit->first < threshold) {
						m_prune.erase(pit, m_prune.end());
						break;
					}
				}
			}

			stop_watch.restart();
			backward();
			time_for_backward += stop_watch.elapsed();

			/// Evaluation
            long double dummy_prob;
			size_t max_z;
			stop_watch.restart();
			vector<size_t> y_seq = viterbiSearch(max_z, dummy_prob);
			assert(y_seq.size() == it->seq.size());
			time_for_evaluation += stop_watch.elapsed();

			/// calculate Y sequence
			long double y_seq_prob = calculateProb(*it);
            if (!std::isfinite((double)y_seq_prob)) {
                cerr << "calculateProb:" << y_seq_prob << endl;
            }

			stop_watch.restart();
						[[maybe_unused]] size_t prev_outcome = m_default_oid;
			vector<size_t> reference, hypothesis;
			for (size_t i = 0; i < it->seq.size(); ++i) {	 /// for each node in sequence
				
				size_t outcome = it->seq[i].label;
				reference.push_back(outcome);
				hypothesis.push_back(y_seq[i]);

				/// calculate the expectation
				/// E[p] - E[~p]

				/// f(y,x)
				vector<ObsParam> obs_param = m_ParamSeq.makeObsIndex(it->seq[i].obs);
				for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					long double prob_sum = 0.0;
					size_t new_y;
					//for (size_t z = 0; z < m_topic_size; z++) {
					for (size_t prune = 0; prune < m_prune.size(); prune++) {
						size_t z = m_prune[prune].second;

						if ((new_y = m_zy_index[z][iter->y]) < m_state_size) { 
							size_t index = TCRF2_MAT2(m_zy_size[z], i, new_y);
							prob_sum += m_Alpha[z][index] * 	m_Beta[z][index] * 	m_Gamma[z] / zval;
						} ///< if
					}
					gradient_seq[iter->fid] += prob_sum * iter->fval * count;
				}

				/// f(y,y)
				if (i > 0) {
					vector<StateParam>::iterator iter = m_ParamSeq.m_StateIndex.begin();
					for (; iter != m_ParamSeq.m_StateIndex.end(); ++iter) {
						long double a_y;
						long double prob_sum = 0.0;
						// for (size_t z = 0; z < m_topic_size; z++) {
						for (size_t prune = 0; prune < m_prune.size(); prune++) {
							size_t z = m_prune[prune].second;

							size_t new_y1, new_y2;
							new_y1 = new_y2 = m_state_size;
							if ( (new_y1 = m_zy_index[z][iter->y1]) < m_state_size && (new_y2 = m_zy_index[z][iter->y2]) < m_state_size) {
								if (i == 0) {
									if (iter->y1 == m_default_oid) a_y = 1.0;
									else a_y = 0.0;
								} else {
									a_y = m_Alpha[z][TCRF2_MAT2(m_zy_size[z], i-1, new_y1)];
								}
								long double b_y = m_Beta[z][TCRF2_MAT2(m_zy_size[z], i, new_y2)];
								long double m_yy = m_R[MAT2(i,iter->y2)] * m_M[MAT2(iter->y1,iter->y2)] * m_Z[MAT2(z, iter->y2)];
								long double prob = a_y * b_y * m_yy * m_Gamma[z] / zval;
								prob_sum += prob;
							} ///< if
						} ///< for z
						gradient_seq[iter->fid] += prob_sum * iter->fval * count;
					} ///< for each edge
				}	///< if ( i > 0)
				prev_outcome = outcome;
				
				/// f(y,z)
				for (vector<StateParam>::iterator iter = m_ParamTopic.m_StateIndex.begin(); iter != m_ParamTopic.m_StateIndex.end(); ++iter) {
					size_t new_y;
					if ( (new_y = m_zy_index[iter->y1][iter->y2]) < m_state_size) {
						size_t index = TCRF2_MAT2(m_zy_size[iter->y1], i, new_y);
						long double prob = m_Alpha[iter->y1][index] * 	m_Beta[iter->y1][index] * m_Gamma[iter->y1] / zval;
						gradient_topic[iter->fid] += prob * iter->fval * count;
					}
				}		

			} ///< for each node in sequence
			
			/// f(z,x)
			vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(it->topic.obs);
			for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
				long double prob = m_Alpha[iter->y][TCRF2_MAT2(m_zy_size[iter->y], m_seq_size-1, m_default_oid)] * m_Gamma[iter->y] / zval;
				gradient_topic[iter->fid] += prob * iter->fval * count;
			}
			
			for (size_t c = 0; c < count; c++) {
				eval2.addLikelihood(y_seq_prob);	/// loglikelihood
				eval2.append(reference, hypothesis);	/// evaluation (accuracy and f1 score)
				vector<size_t> reference1, hypothesis1;
				reference1.push_back(it->topic.label);
				hypothesis1.push_back(max_z);
				eval1.addLikelihood(y_seq_prob);	/// loglikelihood
				eval1.append(reference1, hypothesis1);
			}

			time_for_estimating += stop_watch.elapsed();

		} ///< for m_TrainSet

		/////////////////////////////////////////////////////////////////////////////////
		/// Evaluation for dev set 
		////////////////////////////////////////////////////////////////////////////////
		Evaluator dev_eval1(m_ParamTopic, false);		///< Evaluator (topic)
		Evaluator dev_eval2(m_ParamSeq);		///< Evaluator (sequence)
		dev_eval1.initialize();	///< evaluator intialization
		dev_eval2.initialize(); 
		timer stop_watch;
		[[maybe_unused]] double time_for_dev = 0.0;
		/// for each dev data
        it = m_DevSet.begin();
		count_it = m_DevSetCount.begin();
        for (; it != m_DevSet.end(); ++it, ++count_it) {
			double count = *count_it;
			calculateFactors(*it);
  			forward();
						[[maybe_unused]] long double zval = getPartitionZ();
            long double dummy_prob;
			size_t max_z;
			vector<size_t> y_seq = viterbiSearch(max_z, dummy_prob);
			assert(y_seq.size() == it->seq.size());

						[[maybe_unused]] size_t prev_outcome = m_default_oid;
			vector<size_t> reference, hypothesis;
			for (size_t i = 0; i < it->seq.size(); ++i) {	 /// for each node in sequence
				size_t outcome = it->seq[i].label;
				reference.push_back(outcome);
				hypothesis.push_back(y_seq[i]);
			}
			for (size_t c = 0; c < count; c++) {
				dev_eval2.append(reference, hypothesis);	
				vector<size_t> reference1, hypothesis1;
				reference1.push_back(it->topic.label);
				hypothesis1.push_back(max_z);
				dev_eval1.append(reference1, hypothesis1);
			}

		} ///< for each dev
		time_for_dev = stop_watch.elapsed();

		/// Parameter Merging
		size_t tmp_i = 0;
		for (; tmp_i < m_ParamTopic.size(); ++tmp_i) {
			gradient[tmp_i] = gradient_topic[tmp_i];
		}
		for (size_t i = 0; i < m_ParamSeq.size(); ++i, ++tmp_i) {
			gradient[tmp_i] = gradient_seq[i];
		}

		/// applying regularization
		[[maybe_unused]] size_t n_nonzero = 0;
		if (sigma) {
			if (L1) { /// L1 regularization
				for (size_t i = 0; i < n_theta; ++i) {
					eval2.subLoglikelihood(abs(theta[i] / sigma));
					eval1.subLoglikelihood(abs(theta[i] / sigma));
					if (theta[i] != 0.0) 
						n_nonzero++;
				}
			}
			else {	/// L2 regularization
				n_nonzero = n_theta;
				for (size_t i = 0; i < n_theta; ++i) {
					gradient[i] += theta[i] / sigma;
					eval2.subLoglikelihood((theta[i] * theta[i]) / (2 * sigma));
					eval1.subLoglikelihood((theta[i] * theta[i]) / (2 * sigma));
				}
			}
        }

		/// Checking the end condition 
		double diff = (niter == 0 ? 1.0 : abs(old_obj - eval2.getObjFunc()) / old_obj);
		if (diff < eta) 
			converge++;
		else
			converge = 0;
		old_obj = eval2.getObjFunc();
		if (converge == 3)
			break;
		
		/// LBFGS optimizer
		int ret = lbfgs.optimize(n_theta, theta, eval2.getObjFunc(), gradient, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;
		
		/// Reporting the result
		eval1.calculateF1();
		eval2.calculateF1();
		if (m_DevSet.size() > 0) {
			dev_eval1.calculateF1();
			dev_eval2.calculateF1();
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f  |  %8.3f %8.3f %8.3f\n", 
				niter, eval1.getLoglikelihood(), 
				eval1.getAccuracy(), eval1.getMicroF1()[2], eval1.getMacroF1()[2], t2.elapsed(), 
				dev_eval1.getAccuracy(), dev_eval1.getMicroF1()[2], dev_eval1.getMacroF1()[2]);
			logger->report("%4s %15s %8.3f %8.3f %8.3f %8.3f  |  %8.3f %8.3f %8.3f\n", 
				"", "", 
				eval2.getAccuracy(), eval2.getMicroF1()[2], eval2.getMacroF1()[2], t2.elapsed(), 
				dev_eval2.getAccuracy(), dev_eval2.getMicroF1()[2], dev_eval2.getMacroF1()[2]);
		} else {
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f\n", niter, eval1.getLoglikelihood(), 
				eval1.getAccuracy(), eval1.getMicroF1()[2], eval1.getMacroF1()[2], t2.elapsed());
			logger->report("%4s %15s %8.3f %8.3f %8.3f %8.3f\n", "", "", 
				eval2.getAccuracy(), eval2.getMicroF1()[2], eval2.getMacroF1()[2], t2.elapsed());
		}

		/// Updating the parameter vectors
		m_ParamTopic.setWeight(theta);
		m_ParamSeq.setWeight(&theta[m_ParamTopic.size()]);

	} ///< for iter
	
	delete[] theta;
	delete[] gradient;

	return true;

}

/** Training with Psuedo-likelihood.
	@param max_iter	maximum number of iteration
	@param sigma	Gaussian prior variance
*/
bool TriCRF2::estimateWithPL(size_t max_iter, double sigma, bool L1, double eta) {
	LBFGS lbfgs1, lbfgs2;	///< LBFGS optimizer

	double* theta_topic = m_ParamTopic.getWeight();
	double* theta_seq = m_ParamSeq.getWeight();
	double* gradient_topic = m_ParamTopic.getGradient();
	double* gradient_seq = m_ParamSeq.getGradient();
	Evaluator eval1(m_ParamTopic, false);		///< Evaluator (topic)
	Evaluator eval2(m_ParamSeq);		///< Evaluator (sequence)
	timer t;		///< timer

	/// Reporting
	logger->report("[Parameter estimation]\n");
	logger->report("  Method = \t\tPsuedoLikelihood\n");
	logger->report("  Regularization = \t%s\n", (sigma ? (L1 ? "L1":"L2") : "none"));
	logger->report("  Penalty value = \t%.2f\n\n", sigma);
	logger->report("  >>Parameters for topic features\n");
	m_ParamTopic.print(logger);
	logger->report("  >>Parameters for sequence features\n");
	m_ParamSeq.print(logger);
	logger->report("[Iterations]\n");
	logger->report("%4s %15s %8s %8s %8s %8s\n", "iter", "loglikelihood", "acc", "micro-f1", "macro-f1", "sec");
	
	double old_obj = 1e+37, old_obj2 = 1e+37;
	int converge = 0, converge2 = 0;

	[[maybe_unused]] double time_for_sequence = 0.0;
	[[maybe_unused]] double time_for_topic = 0.0;
	
	createIndex();

	/// Training iteration
    for (size_t niter = 0 ;niter < max_iter; ++niter) {

		/// Initializing local variables
        timer t2;	///< elapsed time for one iteration
		m_ParamTopic.initializeGradient();	///< gradient vector initialization
		m_ParamSeq.initializeGradient();

		eval1.initialize();	///< evaluator intialization
		eval2.initialize(); 

		/// for each training example
		timer stop_watch;
        vector<TriSequence>::iterator it = m_TrainSet.begin();
		vector<double>::iterator count_it = m_TrainSetCount.begin();
        for (; it != m_TrainSet.end(); ++it, ++count_it) {
			double count = *count_it;


			/////////////////////////////////////////////////////////////////////
			/// PL for topic
			/////////////////////////////////////////////////////////////////////
			vector<size_t> reference1, hypothesis1;
			size_t max_z = 0;
			vector<double> prob_topic(m_topic_size);
			fill(prob_topic.begin(), prob_topic.end(), 0.0);

			/// Inference
			vector<ObsParam> obs_param = m_ParamTopic.makeObsIndex(it->topic.obs);
			for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
				prob_topic[iter->y] += theta_topic[iter->fid] * iter->fval;
			}
			for (size_t i = 0; i < it->seq.size(); ++i) {	 /// f(y,z)
				for (vector<StateParam>::iterator iter = m_ParamTopic.m_StateIndex.begin(); iter != m_ParamTopic.m_StateIndex.end(); ++iter) {
					if (iter->y2 == it->seq[i].label)
						prob_topic[iter->y1] += theta_topic[iter->fid] * iter->fval;
				}		
			}
			
			/// normalize
			double sum = 0.0;
			double max = 0.0;
			for (size_t j=0; j < m_topic_size; j++) {
				prob_topic[j] = exp(prob_topic[j]); // * y_prob[j];
				sum += prob_topic[j];
				if (prob_topic[j] > max) {
					max = prob_topic[j];
					max_z = j;
				}
			}
			for (size_t j=0; j < m_topic_size; j++) {
				prob_topic[j] /= sum;
			}
			
			/// evaluating
			reference1.push_back(it->topic.label);
			hypothesis1.push_back(max_z);

			/// calculate the expectation
			/// E[p] - E[~p]
			for(vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
				gradient_topic[iter->fid] += prob_topic[iter->y] * iter->fval * count;
			}
			for (size_t i = 0; i < it->seq.size(); ++i) { /// f(y,z)
				for (vector<StateParam>::iterator iter = m_ParamTopic.m_StateIndex.begin(); iter != m_ParamTopic.m_StateIndex.end(); ++iter) {
					if (iter->y2 == it->seq[i].label)
						gradient_topic[iter->fid] += prob_topic[iter->y1] * iter->fval * count;
				}		
			}
			
			/////////////////////////////////////////////////////////////////////
			/// PL for sequence
			/////////////////////////////////////////////////////////////////////
			size_t prev_label = m_default_oid;
						[[maybe_unused]] size_t next_label = m_default_oid;
			vector<size_t> reference2, hypothesis2;
			for (size_t i = 0; i < it->seq.size(); ++i) {

				size_t max_y = m_default_oid;
				vector<double> prob_seq(m_state_size);
				fill(prob_seq.begin(), prob_seq.end(), 0.0);

				/// w * f (for all classes)
				vector<ObsParam> obs_param = m_ParamSeq.makeObsIndex(it->seq[i].obs);
				for (vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					prob_seq[iter->y] += theta_seq[iter->fid] * iter->fval;
				}
				for (vector<StateParam>::iterator iter = m_ParamSeq.m_StateIndex.begin(); iter != m_ParamSeq.m_StateIndex.end(); ++iter) {
					if (iter->y1 == prev_label)
						prob_seq[iter->y2] += theta_seq[iter->fid] * iter->fval;
				}

				/// normalize
				double sum = 0.0;
				double max = 0.0;
				for (size_t j=0; j < m_state_size; j++) {
					prob_seq[j] = exp(prob_seq[j]); // * y_prob[j];
					sum += prob_seq[j];
					if (prob_seq[j] > max) {
						max = prob_seq[j];
						max_y = j;
					}
				}
				for (size_t j=0; j < m_state_size; j++) {
					prob_seq[j] /= sum;
				}
				
				reference2.push_back(it->seq[i].label);
				hypothesis2.push_back(max_y);

				for (vector<ObsParam>::iterator iter = obs_param.begin(); iter != obs_param.end(); ++iter) {
					gradient_seq[iter->fid] += prob_seq[iter->y] * iter->fval * count;
				}
				for (vector<StateParam>::iterator iter = m_ParamSeq.m_StateIndex.begin(); iter != m_ParamSeq.m_StateIndex.end(); ++iter) {
					if (iter->y1 == prev_label)
						gradient_seq[iter->fid] = prob_seq[iter->y2] * iter->fval * count;
				}

				/// evaluation (accuracy and f1 score)
				for (size_t c = 0; c < count; c++) {
					eval2.addLikelihood(prob_seq[it->seq[i].label]);	
				}

				prev_label = it->seq[i].label;
			}

			/// evaluation
			for (size_t c = 0; c < count; c++) {
				eval1.addLikelihood(prob_topic[it->topic.label]);	
				eval1.append(reference1, hypothesis1);
				eval2.append(reference2, hypothesis2);
			}

		} ///< for m_TrainSet
		time_for_topic += stop_watch.elapsed();

		/////////////////////////////////////////////////////////////////////////////////
		/// Evaluation for dev set 
		////////////////////////////////////////////////////////////////////////////////
		Evaluator dev_eval1(m_ParamTopic, false);		///< Evaluator (topic)
		Evaluator dev_eval2(m_ParamSeq);		///< Evaluator (sequence)
		dev_eval1.initialize();	///< evaluator intialization
		dev_eval2.initialize(); 
		stop_watch.restart();
		[[maybe_unused]] double time_for_dev = 0.0;
		/// for each dev data
        it = m_DevSet.begin();
		count_it = m_DevSetCount.begin();
        for (; it != m_DevSet.end(); ++it, ++count_it) {
			double count = *count_it;
			calculateFactors(*it);
  			forward();
						[[maybe_unused]] long double zval = getPartitionZ();
            long double dummy_prob;
			size_t max_z;
			vector<size_t> y_seq = viterbiSearch(max_z, dummy_prob);
			assert(y_seq.size() == it->seq.size());

						[[maybe_unused]] size_t prev_outcome = m_default_oid;
			vector<size_t> reference, hypothesis;
			for (size_t i = 0; i < it->seq.size(); ++i) {	 /// for each node in sequence
				size_t outcome = it->seq[i].label;
				reference.push_back(outcome);
				hypothesis.push_back(y_seq[i]);
			}
			for (size_t c = 0; c < count; c++) {
				dev_eval2.append(reference, hypothesis);	
				vector<size_t> reference1, hypothesis1;
				reference1.push_back(it->topic.label);
				hypothesis1.push_back(max_z);
				dev_eval1.append(reference1, hypothesis1);
			}

		} ///< for each dev
		time_for_dev = stop_watch.elapsed();

		/// Parameter Merging
		/*
		size_t tmp_i = 0;
		for (; tmp_i < m_ParamTopic.size(); ++tmp_i) {
			gradient[tmp_i] = gradient_topic[tmp_i];
		}
		for (size_t i = 0; i < m_ParamSeq.size(); ++i, ++tmp_i) {
			gradient[tmp_i] = gradient_seq[i];
		}*/

		/// applying regularization
		[[maybe_unused]] size_t n_nonzero = 0;
		if (sigma) {
			if (L1) { /// L1 regularization
				for (size_t i = 0; i < m_ParamTopic.size(); ++i) {
					eval1.subLoglikelihood(abs(theta_topic[i] / sigma));
					if (theta_topic[i] != 0.0) 
						n_nonzero++;
				}
				for (size_t i = 0; i < m_ParamSeq.size(); ++i) {
					eval2.subLoglikelihood(abs(theta_seq[i] / sigma));
					if (theta_seq[i] != 0.0) 
						n_nonzero++;
				}
			}
			else {	/// L2 regularization
				n_nonzero = m_ParamTopic.size() + m_ParamSeq.size();
				for (size_t i = 0; i < m_ParamTopic.size(); ++i) {
					gradient_topic[i] += theta_topic[i] / sigma;
					eval1.subLoglikelihood((theta_topic[i] * theta_topic[i]) / (2 * sigma));
				}
				for (size_t i = 0; i < m_ParamSeq.size(); ++i) {
					gradient_seq[i] += theta_seq[i] / sigma;
					eval2.subLoglikelihood((theta_seq[i] * theta_seq[i]) / (2 * sigma));
				}
			}
        }

		/// Checking the end condition 
		double diff = (niter == 0 ? 1.0 : abs(old_obj - eval1.getObjFunc()) / old_obj);
		if (diff < eta) 
			converge++;
		else
			converge = 0;
		old_obj = eval1.getObjFunc();
		if (converge == 3)
			break;

		double diff2 = (niter == 0 ? 1.0 : abs(old_obj2 - eval2.getObjFunc()) / old_obj2);
		if (diff2 < eta) 
			converge2++;
		else
			converge2 = 0;
		old_obj2 = eval2.getObjFunc();
		if (converge2 == 3)
			break;

		
		/// LBFGS optimizer
		int ret = lbfgs1.optimize(m_ParamTopic.size(), theta_topic, eval1.getObjFunc(), gradient_topic, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;

		ret = lbfgs2.optimize(m_ParamSeq.size(), theta_seq, eval2.getObjFunc(), gradient_seq, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;

		
		/// Reporting the result
		eval1.calculateF1();
		eval2.calculateF1();
		if (m_DevSet.size() > 0) {
			dev_eval1.calculateF1();
			dev_eval2.calculateF1();
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f  |  %8.3f %8.3f %8.3f\n", 
				niter, eval1.getLoglikelihood(), 
				eval1.getAccuracy(), eval1.getMicroF1()[2], eval1.getMacroF1()[2], t2.elapsed(), 
				dev_eval1.getAccuracy(), dev_eval1.getMicroF1()[2], dev_eval1.getMacroF1()[2]);
			logger->report("%4s %15s %8.3f %8.3f %8.3f %8.3f  |  %8.3f %8.3f %8.3f\n", 
				"", "", 
				eval2.getAccuracy(), eval2.getMicroF1()[2], eval2.getMacroF1()[2], t2.elapsed(), 
				dev_eval2.getAccuracy(), dev_eval2.getMicroF1()[2], dev_eval2.getMacroF1()[2]);
		} else {
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f\n", niter, eval1.getLoglikelihood(), 
				eval1.getAccuracy(), eval1.getMicroF1()[2], eval1.getMacroF1()[2], t2.elapsed());
			logger->report("%4s %15E %8.3f %8.3f %8.3f %8.3s\n", "", eval2.getLoglikelihood(), 
				eval2.getAccuracy(), eval2.getMicroF1()[2], eval2.getMacroF1()[2], "");
		}

		/// Updating the parameter vectors
//		m_ParamTopic.setWeight(theta);
//		m_ParamSeq.setWeight(&theta[m_ParamTopic.size()]);

	} ///< for iter
	
	//delete theta;
	//delete gradient;

	return true;

}

bool TriCRF2::pretrain(size_t max_iter, double sigma, bool L1) { 
		return estimateWithPL(max_iter, sigma, L1); 
}

bool TriCRF2::train(size_t max_iter, double sigma, bool L1) { 
		return estimateWithLBFGS(max_iter, sigma, L1);
}

bool TriCRF2::test(const std::string& filename, const std::string& outputfile, [[maybe_unused]] bool confidence) {
	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");

	/// output
	ofstream out;
	vector<string> state_vec, seq_state_vec;
	if (outputfile != "") {
		out.open(outputfile.c_str());
		out.precision(20);
		state_vec = m_ParamTopic.getState().second;
		seq_state_vec = m_ParamSeq.getState().second;
	}

	/// initializing
	size_t count = 0;
	TriStringSequence triseq;
	logger->report("[Testing begins ...]\n");
	timer stop_watch;
	Evaluator test_eval1(m_ParamTopic, false);		///< Evaluator (topic)
	Evaluator test_eval2(m_ParamSeq);		///< Evaluator (sequence)
	test_eval1.initialize();	///< evaluator intialization
	test_eval2.initialize(); 
	size_t seq_count = 0;

	calculateEdge();

	/// reading the text
	while (getline(f,line)) {
		vector<string> tokens = tokenize(line, " \t");
		if (line.empty()) {
			/// test
			calculateFactors(triseq);
  			forward();
						[[maybe_unused]] long double zval = getPartitionZ();
            long double dummy_prob;

			////////////////////////////////////////////////////////////////////
			/// pruning
			////////////////////////////////////////////////////////////////////
			long double threshold = m_prune[0].first / m_prune_threshold;
			vector<pair<long double, size_t> >::iterator pit = m_prune.begin();
			for (; pit != m_prune.end(); pit++) {
				if (pit->first < threshold) {
					m_prune.erase(pit, m_prune.end());
					break;
				}
			}

			size_t max_z;
			vector<size_t> y_seq = viterbiSearch(max_z, dummy_prob);
			assert(y_seq.size() == triseq.size());

			vector<size_t> reference1, hypothesis1;
			reference1.push_back(triseq.topic.label);
			hypothesis1.push_back(max_z);
			test_eval1.append(reference1, hypothesis1);
			if (outputfile != "") {
				out << state_vec[max_z];
				/*
				if (confidence) {
					double prob = m_Alpha[max_z][TCRF2_MAT2(m_zy_size[max_z], m_seq_size-1, m_y_state[max_z][0].y1)] * m_Gamma[max_z] / zval;
					out << " " << prob;
				}
				*/
				out << endl;
			}
			
						[[maybe_unused]] size_t prev_y = m_default_oid;
			vector<string> reference, hypothesis;
			StringSequence::iterator it = triseq.seq.begin();
			for (size_t i = 0; it != triseq.seq.end(); ++i, ++it) {	 /// for each node in sequence
				size_t outcome = triseq.seq[i].label;
				string outcome_s;
				if (m_ParamSeq.sizeStateVec() <= outcome) 
					outcome_s = m_ParamSeq.getState().second[m_default_oid];
				else
					outcome_s = m_ParamSeq.getState().second[outcome];
				string y_seq_s = m_ParamSeq.getState().second[y_seq[i]];

				reference.push_back(outcome_s);
				hypothesis.push_back(y_seq_s);

				if (outputfile != "") {
					out << y_seq_s;
					/*
					if (confidence) {
						double norm = 0.0;
						for (size_t j = 0; j < m_y_state.size(); j++)
							norm += m_R[MAT2(i, j)] * m_M[MAT2(prev_y, j)] * m_Z[MAT2(max_z, j)]; 
						double prob = m_R[MAT2(i, y_seq[i])] * m_M[MAT2( prev_y, y_seq[i])] * m_Z[MAT2(max_z, y_seq[i])] / norm;
						out << " " << prob;
						prev_y = y_seq[i];
					}
					*/
					out << endl; 
				}
			}
			if (outputfile != "")
				out << endl;

			test_eval2.append(m_ParamSeq, reference, hypothesis);	

			triseq.seq.clear();
			seq_count = 0;
			++count;
		} else {
			++seq_count;
			if (seq_count == 1) { ///< this is a topic 
				triseq.topic = packEvent(tokens, &m_ParamTopic, true);	///< wanrning: There are no common element in topic classes and sequence classes.
			} else {
				StringEvent ev = packStringEvent(tokens, &m_ParamSeq, true);	///< observation features
				triseq.seq.push_back(ev);	///< append
			}

		}	///< else
		
		
	}	///< while

	test_eval1.calculateF1();
	test_eval2.calculateF1();
	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  testing time = \t%.3f\n\n", stop_watch.elapsed());
	logger->report("  Topic Classification \n");
	logger->report("  Acc = \t\t%8.3f\n", test_eval1.getAccuracy());
	logger->report("  MicroF1 = \t\t%8.3f\n", test_eval1.getMicroF1()[2]);
	logger->report("  MacroF1 = \t\t%8.3f\n", test_eval1.getMacroF1()[2]);
	logger->report("  Sequential Labeling\n");
	logger->report("  Acc = \t\t%8.3f\n", test_eval2.getAccuracy());
	logger->report("  MicroF1 = \t\t%8.3f\n", test_eval2.getMicroF1()[2]);
	logger->report("  MacroF1 = \t\t%8.3f\n", test_eval2.getMacroF1()[2]);
	
	return true;
}

}	///< namespace tricrf
