/*
 * Copyright (C) 2010 Minwoo Jeong (minwoo.j@gmail.com).
 * This file is part of the "TriCRF" distribution.
 * http://github.com/minwoo/TriCRF/
 * This software is provided under the terms of Modified BSD license: see LICENSE for the detail.
 */

/// max headers
#include "MaxEnt.h"
#include "Evaluator.h"
#include "Utility.h"
#include "LBFGS.h"
/// standard headers
#include <cassert>
#include <cfloat>
#include <cmath>
#include <limits>
#include <algorithm>
#include <stdexcept>
#include <iostream>
#include <fstream>

#define MAT3(I,X,Y)    ((n_outcome * n_outcome * (I)) + (n_outcome * (X)) + Y)
#define MAT2(I,X)    ((n_outcome * (I)) + X)

using namespace std;

namespace tricrf {

/// Constructor
MaxEnt::MaxEnt() {
	logger = new Logger();
}

MaxEnt::MaxEnt(Logger *logger_ptr) {
	setLogger(logger_ptr);
	logger->report(2, MAX_HEADER);
	logger->report(2, ">> Maximum Entropy << \n\n");
}

void MaxEnt::setLogger(Logger *logger_ptr) { 
	logger = logger_ptr;
}

void MaxEnt::setPrune(double prune) {
	m_prune_threshold = prune;
}

/// Deconstructor
MaxEnt::~MaxEnt() {
	// Note: Logger cleanup is handled by RAII in Logger class
	// We don't delete logger here as it may be shared between instances
	// or managed externally by the caller
}

void MaxEnt::clear() {
	m_Param.clear();
}

/** Save the model.
	@param filename file to be saved 
	@return success or fail
*/
bool	MaxEnt::saveModel(const std::string& filename) {
	/// Checking the error
	if (filename == "")
		return false;

	timer stop_watch;
	logger->report("[Model saving]\n");

	/// file stream
    ofstream f(filename.c_str());
    f.precision(20);
    if (!f)
        throw runtime_error("unable to open file to write");

    /// header
    f << "# MAX: A C++ Library for Structured Prediction" << endl;
	f << "# MaxEnt Model file (text format)" << endl;
	f << "# Do not edit this file" << endl;
	f << "# " << endl << ":" << endl;
	
	bool ret = m_Param.save(f);
	f.close();
	logger->report("  saving time = \t%.3f\n\n", stop_watch.elapsed());

	return ret;
}

/** Load the model.
	@param filename file to be loaded
	@return success or fail
*/
bool MaxEnt::loadModel(const std::string& filename) {
	/// Checking the error
	if (filename == "")
		return false;

	timer stop_watch;
	logger->report("[Model loading]\n");

	/// file stream
    ifstream f(filename.c_str());
    f.precision(20);
    if (!f)
        throw runtime_error("fail to open model file");

    /// header
	size_t count = 0;
    string line;
    getline(f, line);
    while (line.empty() || line[0] == '#') {
		if (count == 1) {
			vector<string> tok = tokenize(line);
			if (tok.size() < 2 || tok[1] != "MaxEnt") {
				logger->report("|Error| Invalid model files ... \n");
				return false;
			}
		}
        getline(f, line);
		count++;
	}

	bool ret = m_Param.load(f);
	f.close();
	m_Param.print(logger);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());

	return ret;
}

/**	Add an event to memory.
	@param tokens	string tokens to be packed
	@param p_Param	parameter pointer
*/
Event MaxEnt::packEvent(vector<string>& tokens, Parameter* p_Param, bool test) {
	Event ev;		///< Event
	vector<string>::iterator it = tokens.begin();

	if (!p_Param)	///< for generalization
		p_Param = &m_Param;

	/// label confidence
	/// todo: this can be used for cascading system.
	string fstr(it->c_str());
	vector<string> tok = tokenize(fstr, ":");
	double fval = 1.0;
	if (tok.size() > 1) {
		fval = atof(tok[1].c_str());	///< feature value
		fstr = tok[0];
	}
	
	if (!test) { ///< train data
		ev.label = p_Param->addNewState(fstr);	// outcome id
	} else { ///< dev, test data
		int oid;
		if ( (oid = p_Param->findState(fstr)) >= 0 )
			ev.label = p_Param->findState(fstr);
		else
			ev.label = p_Param->sizeStateVec();
	}
	ev.fval = fval;
	
	// temporary (for transfer learning)
	//if (!test && fstr == "O" && fval < 1.0)
	//	test = true;
	
	// observation
	++it;
	for (; it != tokens.end();) {
		string fi(it->c_str()); ++it;
		vector<string> tok = tokenize(fi, ":");
		string fstr = fi;
		double fval = ev.fval;
		if (tok.size() > 1) {
			fval = atof(tok[1].c_str());	///< feature value
			fstr = tok[0];
		}
		if (!test) {	 ///< train data
			size_t pid = p_Param->addNewObs(fstr);
			ev.obs.push_back(make_pair(pid, 1.0));
			
			p_Param->updateParam(ev.label, pid, fval);
		} else {	 ///< dev, test data
			int pid;
			if ( (pid = p_Param->findObs(fstr)) >= 0 ) {
				ev.obs.push_back(make_pair((size_t)pid, 1.0));
			}
		}
	}
	return ev;
}

Event MaxEnt::packEvent2(vector<string>& tokens, Parameter* p_Param, bool test) {
	Event ev;		///< Event
	vector<string>::iterator it = tokens.begin();

	if (!p_Param)	///< for generalization
		p_Param = &m_Param;

	/// label confidence
	/// todo: this can be used for cascading system.
	string fstr(it->c_str());
	vector<string> tok = tokenize(fstr, ":");
	double fval = 1.0;
	if (tok.size() > 1) {
		fval = atof(tok[1].c_str());	///< feature value
		fstr = tok[0];
	}
	
	if (!test) { ///< train data
		ev.label = p_Param->addNewState(fstr);	// outcome id
	} else { ///< dev, test data
		int oid;
		if ( (oid = p_Param->findState(fstr)) >= 0 )
			ev.label = p_Param->findState(fstr);
		else
			ev.label = p_Param->sizeStateVec();
	}
	ev.fval = fval;

	// temporary (for transfer learning)
	//if (!test && fstr == "O" && fval < 1.0)
	//	test = true;
		
	// observation
	++it;
	for (; it != tokens.end();) {
		string fi(it->c_str()); ++it;
		vector<string> tok = tokenize(fi, ":");
		string fstr = fi;
		double fval = ev.fval;
		if (tok.size() > 1) {
			fval = atof(tok[1].c_str());	///< feature value
			fstr = tok[0];
		}
		
		if (!test) {	 ///< train data
			size_t pid = p_Param->addNewObs(fstr);
			ev.obs.push_back(make_pair(pid, 1.0));
			
			for (size_t i = 0; i < p_Param->sizeStateVec(); i++) 
			{
				if (i == ev.label)
					p_Param->updateParam(ev.label, pid, fval);
				else
					p_Param->updateParam(i, pid, 0.0);				
			}
			
			//p_Param->updateParam(ev.label, pid, fval);
		} else {	 ///< dev, test data
			int pid;
			if ( (pid = p_Param->findObs(fstr)) >= 0 ) {
				ev.obs.push_back(make_pair((size_t)pid, 1.0));
			}
		}
	}
	return ev;
}

/**	Add an event to memory.
	@param tokens	string tokens to be packed
	@param p_Param	parameter pointer
*/
StringEvent MaxEnt::packStringEvent(vector<string>& tokens, Parameter* p_Param, bool test) {
	StringEvent ev;		///< Event
	vector<string>::iterator it = tokens.begin();

	if (!p_Param)	///< for generalization
		p_Param = &m_Param;

	/// label confidence
	/// todo: this can be used for cascading system.
	string fstr(it->c_str());
	vector<string> tok = tokenize(fstr, ":");
	double fval = 1.0;
	if (tok.size() > 1) {
		fval = atof(tok[1].c_str());	///< feature value
		fstr = tok[0];
	}

	if (!test) { ///< train data
		ev.label = p_Param->addNewState(fstr);	// outcome id
	} else { ///< dev, test data
		int oid;
		if ( (oid = p_Param->findState(fstr)) >= 0 )
			ev.label =(size_t)oid;
		else
			ev.label = p_Param->sizeStateVec();
	}
	ev.fval = fval;

	// (for transfer learning)
	//if (!test && fstr == "O" && fval < 1.0)
	//	test = true;
		
	// observation
	++it;
	for (; it != tokens.end();) {
		string fi(it->c_str()); ++it;
		vector<string> tok = tokenize(fi, ":");
		string fstr = fi;
		double fval = ev.fval;
		if (tok.size() > 1) {
			fval = atof(tok[1].c_str());	///< feature value
			fstr = tok[0];
		}
		if (!test) { ///< train data
			size_t pid = p_Param->addNewObs(fstr);
			ev.obs.push_back(make_pair(fstr, 1.0));
			
			/*
			for (size_t i = 0; i < p_Param->sizeStateVec(); i++) 
			{
				if (i == ev.label)
					p_Param->updateParam(ev.label, pid, fval);
				else
					p_Param->updateParam(i, pid, 0.0);				
			}
			*/
			
			p_Param->updateParam(ev.label, pid, fval);
		} else { ///< dev, test data
			int pid;
			if ( (pid = p_Param->findObs(fstr)) >= 0 ) {
				ev.obs.push_back(make_pair(fstr, 1.0));
			}
		}
	}
	return ev;
}

/** Read training data from file.
*/
void MaxEnt::readTrainData(const string& filename) {

	// initializing
	m_TrainSet.clear();
	m_TrainSetCount.clear();
	map<vector<vector<string> >, size_t> train_data_map;	///<	To reduce the storage and computation
	vector<vector<string> > token_list;

	/// file stream
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");
	string line;
	size_t count = 0;
	Sequence seq;

	/// reading the text
	logger->report("[Training data file loading]\n");
	timer stop_watch;
	while (getline(f,line)) {
		if (line.empty()) {
			if (train_data_map.find(token_list) == train_data_map.end()) {
				m_TrainSet.append(seq);
				train_data_map.insert(make_pair(token_list, m_TrainSetCount.size()));
				m_TrainSetCount.push_back(1.0);
			} else {
				m_TrainSetCount[train_data_map[token_list]] += 1.0;
			}
			seq.clear();
			token_list.clear();
			++count;
		} else {
			vector<string> tokens = tokenize(line);
			seq.push_back(packEvent(tokens));

			token_list.push_back(tokens);
		}	///< else

	}	///< while

	m_Param.endUpdate();

	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());
}

/**	Read the data from file
*/
void MaxEnt::readDevData(const string& filename) {

	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");
	
	/// initializing
	size_t count = 0;
	Sequence seq;
	logger->report("[Dev data file loading]\n");
	timer stop_watch;
	m_DevSet.clear();
	m_DevSetCount.clear();

	///<	To reduce the storage and computation
	map<vector<vector<string> >, size_t> dev_data_map;	
	vector<vector<string> > token_list;

	/// reading the text
	while (getline(f,line)) {
		if (line.empty()) {
			if (dev_data_map.find(token_list) == dev_data_map.end()) {
				m_DevSet.append(seq);
				dev_data_map.insert(make_pair(token_list, m_DevSetCount.size()));
				m_DevSetCount.push_back(1.0);
			} else {
				m_DevSetCount[dev_data_map[token_list]] += 1.0;
			}
			seq.clear();
			token_list.clear();
			++count;
		} else {
			vector<string> tokens = tokenize(line);
			seq.push_back(packEvent(tokens, &m_Param, true));

			token_list.push_back(tokens);
		}	///< else

	}	///< while

	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  loading time = \t%.3f\n\n", stop_watch.elapsed());
}

/** Evaluate the model. 
*/
vector<double> MaxEnt::evaluate(Event ev, size_t& max_outcome) {
	double* theta = m_Param.getWeight();

	vector<double> q(m_Param.sizeStateVec());
	fill(q.begin(), q.end(), 0.0);

	/// w * f (for all classes)
	vector<ObsParam> obs_param = m_Param.makeObsIndex(ev.obs);
	vector<ObsParam>::iterator iter = obs_param.begin();
	for(; iter != obs_param.end(); ++iter) {
		q[iter->y] += theta[iter->fid] * iter->fval;
	}
	
	/// normalize
	double sum = 0.0;
	double max = 0.0;
	for (size_t j=0; j < m_Param.sizeStateVec(); j++) {
		q[j] = exp(q[j]); // * y_prob[j];
		sum += q[j];
		if (q[j] > max) {
			max = q[j];
			max_outcome = j;
		}
	}
	for (size_t j=0; j < m_Param.sizeStateVec(); j++) {
		q[j] /= sum;
	}
	
	return q;	
}

/** Training with LBFGS optimizer.
	@param max_iter	maximum number of iteration
	@param sigma		Gaussian prior variance
	@param L1				using L1 regularization
	@param eta			condition for finishing the iteration 
	@reference	
		1) R. Malouf, 2002, A comparison of algorithms for maximum entropy parameter estimation, CoNLL, pp. 49-55.
		2) F. Sha and F. Pereira, 2003, Shallow parsing with conditional random fields, HLT.
		3) J. Nocedal and S. J. Wright, 1999, Numerical optimization, Springer, New York.
*/
bool MaxEnt::estimateWithLBFGS(size_t max_iter, double sigma, bool L1, double eta) {
	LBFGS lbfgs;	///< LBFGS optimizer
	double* theta = m_Param.getWeight();
	double* gradient = m_Param.getGradient();

	Evaluator eval(m_Param);	///< Evaluator
	timer t;		///< timer

	/// Reporting
	logger->report("[Parameter estimation]\n");
	logger->report("  Method = \t\tLBFGS\n");
	logger->report("  Regularization = \t%s\n", (sigma ? (L1 ? "L1":"L2") : "none"));
	logger->report("  Penalty value = \t%.2f\n", sigma);
	m_Param.print(logger);

	logger->report("[Iterations]\n");
	logger->report("%4s %15s %8s %8s %8s %8s\n", "iter", "loglikelihood", "acc", "micro-f1", "macro-f1", "sec");
	
	double old_obj = 1e+37;
	int converge = 0;

	/// Training iteration
    for (size_t niter = 0 ;niter < max_iter; ++niter) {
		/// Initializing local variables
        timer t2;	///< elapsed time for one iteration
		m_Param.initializeGradient();	///< gradient vector initialization
		eval.initialize();	///< evaluator intialization
		
		/// for each training set
        vector<Sequence>::iterator sit = m_TrainSet.begin();
		vector<double>::iterator count_it = m_TrainSetCount.begin();
        for (; sit != m_TrainSet.end(); ++sit, ++count_it) {
			Sequence::iterator it = sit->begin();
			double count = *count_it;
			vector<size_t> reference, hypothesis;

			for (; it != sit->end(); ++it) {	 /// for each node
				/// evaluation 
				size_t max_outcome = 0;
				vector<double> q = evaluate(*it, max_outcome);

				reference.push_back(it->label);
				hypothesis.push_back(max_outcome);

				/// calculate the expectation
				/// E[p] - E[~p]
				vector<ObsParam> obs_param = m_Param.makeObsIndex(it->obs);
				vector<ObsParam>::iterator iter = obs_param.begin();
				for(; iter != obs_param.end(); ++iter) {
					gradient[iter->fid] += q[iter->y] * iter->fval * count;
				}
		
				/// loglikelihood
				for (size_t c = 0; c < count; c++) {
					eval.addLikelihood(q[it->label]);	
				}
		
			} ///< for sequence
			/// evaluation (accuracy and f1 score)
			for (size_t c = 0; c < count; c++) {
				eval.append(reference, hypothesis);
			}
			
		} ///< for m_TrainSet

		/////////////////////////////////////////////////////////////////////////////////
		/// Evaluation for dev set 
		////////////////////////////////////////////////////////////////////////////////
		Evaluator dev_eval(m_Param);						///< Evaluator
		dev_eval.initialize();										///< Evaluator intialization
		
		/// Timer for dev set evaluation
		timer stop_watch;
		[[maybe_unused]] double time_for_dev = 0.0;
		/// for each dev data
        sit = m_DevSet.begin();
		count_it = m_DevSetCount.begin();
        for (; sit != m_DevSet.end(); ++sit, ++count_it) {
			Sequence::iterator it = sit->begin();
			double count = *count_it;
			vector<size_t> reference, hypothesis;
			for (; it != sit->end(); ++it) {	 /// for each node
				/// evaluation 
				size_t max_outcome = 0;
				vector<double> q = evaluate(*it, max_outcome);

				reference.push_back(it->label);
				hypothesis.push_back(max_outcome);
			}
			for (size_t c = 0; c < count; c++) {
				dev_eval.append(reference, hypothesis);	
			}
		} ///< for each dev
		time_for_dev = stop_watch.elapsed();

		/// applying regularization
		[[maybe_unused]] size_t n_nonzero = 0;
		if (sigma) {
			if (L1) { /// L1 regularization
				for (size_t i = 0; i < m_Param.size(); ++i) {
					eval.subLoglikelihood(abs(theta[i] / sigma));
					if (theta[i] != 0.0) 
						n_nonzero++;
				}
			}
			else {	/// L2 regularization
				n_nonzero = m_Param.size();
				for (size_t i = 0; i < m_Param.size(); ++i) {
					gradient[i] += theta[i] / sigma;
					eval.subLoglikelihood((theta[i] * theta[i]) / (2 * sigma));
				}
			}
        }
		
		double diff = (niter == 0 ? 1.0 : abs(old_obj - eval.getObjFunc()) / old_obj);
		if (diff < eta) 
			converge++;
		else
			converge = 0;
		old_obj = eval.getObjFunc();
		if (converge == 3)
			break;

		/// LBFGS optimizer
		int ret = lbfgs.optimize(m_Param.size(), theta, eval.getObjFunc(), gradient, L1, sigma);
		if (ret < 0)
			return false;
		else if (ret == 0)
			return true;
		
		eval.calculateF1();
		if (m_DevSet.size() > 0) {
			dev_eval.calculateF1();
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f  |  %8.3f %8.3f %8.3f\n", 
				niter, eval.getLoglikelihood(), 
				eval.getAccuracy(), eval.getMicroF1()[2], eval.getMacroF1()[2], t2.elapsed(), 
				dev_eval.getAccuracy(), dev_eval.getMicroF1()[2], dev_eval.getMacroF1()[2]);
		} else {
			logger->report("%4d %15E %8.3f %8.3f %8.3f %8.3f\n", niter, eval.getLoglikelihood(),
				eval.getAccuracy(), eval.getMicroF1()[2], eval.getMacroF1()[2], t2.elapsed());
		}

	} ///< for iter

	return true;
}

void MaxEnt::initializeModel() {
	m_Param.initialize();
}

bool MaxEnt::pretrain([[maybe_unused]] size_t max_iter, [[maybe_unused]] double sigma, [[maybe_unused]] bool L1) { 
	return 1; 
}

bool MaxEnt::train(size_t max_iter, double sigma, bool L1) { 
	return estimateWithLBFGS(max_iter, sigma, L1); 
}

bool MaxEnt::test(const std::string& filename, const std::string& outputfile, bool confidence) {
	/// File stream
	string line;
	ifstream f(filename.c_str());
	if (!f)
		throw runtime_error("cannot open data file");
	
	/// output
	ofstream out;
	vector<string> state_vec;
	if (outputfile != "") {
		out.open(outputfile.c_str());
		out.precision(20);
		state_vec = m_Param.getState().second;
	}
	
	/// initializing
	size_t count = 0;
	Sequence seq;
	logger->report("[Testing begins ...]\n");
	timer stop_watch;
	Evaluator test_eval(m_Param);						///< Evaluator
	test_eval.initialize();										///< Evaluator intialization

	/// reading the text
	while (getline(f,line)) {
		if (line.empty()) {
			/// test
			vector<size_t> reference, hypothesis;
			Sequence::iterator it = seq.begin();
			for (; it != seq.end(); ++it) {	 /// for each node
				/// evaluation 
				size_t max_outcome = 0;
				vector<double> q = evaluate(*it, max_outcome);

				reference.push_back(it->label);
				hypothesis.push_back(max_outcome);
				if (outputfile != "") {
					out << state_vec[max_outcome];
					if (confidence)
						out << " " << q[max_outcome];
					out << endl; 
				}
			}
			if (outputfile != "")
				out << endl;
			test_eval.append(reference, hypothesis);	
			seq.clear();
			++count;
		} else {
			vector<string> tokens = tokenize(line);
			seq.push_back(packEvent(tokens, &m_Param, true));
		}	///< else
	}	///< while

	test_eval.calculateF1();
	logger->report("  # of data = \t\t%d\n", count);
	logger->report("  testing time = \t%.3f\n\n", stop_watch.elapsed());
	logger->report("  Acc = \t\t%8.3f\n", test_eval.getAccuracy());
	logger->report("  MicroF1 = \t\t%8.3f\n", test_eval.getMicroF1()[2]);
	logger->report("  MacroF1 = \t\t%8.3f\n", test_eval.getMacroF1()[2]);
	
	return true;
}


}	///< namespace tricrf

